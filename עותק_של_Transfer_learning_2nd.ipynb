{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "עותק של Transfer_learning_2nd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etgins/Mice_ASD_Detection/blob/main/%D7%A2%D7%95%D7%AA%D7%A7_%D7%A9%D7%9C_Transfer_learning_2nd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu8kZrydfX4g"
      },
      "source": [
        "Fetching files from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oo5j-p0b9ja",
        "outputId": "aab0860b-4178-4575-b590-d1424d900345"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/Shareddrives/USV_RECORDINGS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/content/drive/Shareddrives/USV_RECORDINGS'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwCw8XreUK51",
        "outputId": "78c1b671-991b-44ee-8057-825d4400a731"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3IiGrIPTmo3",
        "outputId": "604584c0-9fc2-4dd2-9b96-f0dac9e467d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS7nvgaAfmJw"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK0ma5tqcUJI"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "import random\n",
        "from tensorflow.keras import layers, models\n",
        "from keras.layers import UpSampling2D,Input,LeakyReLU,MaxPooling2D,Dropout,Lambda,add,Activation,Concatenate,Dense,Flatten,Conv2D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# from keras.utils import plot_model\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkUDZO-DfsBO"
      },
      "source": [
        "Functions Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBqG2KX7dr6S"
      },
      "source": [
        "def Remove_None_Data_and_lowering_Labels(data,labels):\n",
        "    data = [j for i,j in enumerate(data) if j is not None] \n",
        "    labels= [j for i,j in enumerate(labels) if j is not None] \n",
        "    labels = [str(i) for i in labels]\n",
        "    labels = [i.lower() for i in labels]\n",
        "    return data, labels\n",
        "    \n",
        "def Take_Syllables(Syllable_name,Data,Labels):\n",
        "    ind = Labels == Syllable_name.lower()\n",
        "    Syllables = Data[ind]\n",
        "    return Syllables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u_L2eRGfTXP"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dSO47qFd6BT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "f6e6ea6f-ef0d-4ac7-e080-95f0ca1ec036"
      },
      "source": [
        "#img_rows, img_cols = 64, 64 #choosing resize values\n",
        "Data_united = np.load('Data_united_96_18072021.npy',allow_pickle = True)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "Data_united = np.array([scaler.fit_transform(i) for i in Data_united])\n",
        "\n",
        "Labels_united = np.load('Labels_united_18072021.npy',allow_pickle = True)\n",
        "Data_united = np.repeat(Data_united[:, :, :, np.newaxis], 3, -1)\n",
        "\n",
        "Complex_syl = Take_Syllables('Complex',Data_united,Labels_united)\n",
        "Frequency_steps_syl = Take_Syllables('Frequency steps',Data_united,Labels_united)\n",
        "Chevron_syl = Take_Syllables('Chevron',Data_united,Labels_united)\n",
        "Composite_syl = Take_Syllables('Composite',Data_united,Labels_united)\n",
        "Noise_syl = Take_Syllables('99.0',Data_united,Labels_united)\n",
        "two_syllable_syl = Take_Syllables('two-syllable',Data_united,Labels_united)\n",
        "upward_syl = Take_Syllables('upward',Data_united,Labels_united)\n",
        "flat_syl = Take_Syllables('flat',Data_united,Labels_united)\n",
        "short_syl = Take_Syllables('short',Data_united,Labels_united)\n",
        "harmonics_syl = Take_Syllables('harmonics',Data_united,Labels_united)\n",
        "downward_syl = Take_Syllables('downward',Data_united,Labels_united)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-236f2a345417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#img_rows, img_cols = 64, 64 #choosing resize values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mData_united\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data_united_96_18072021.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_united_96_18072021.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d81QjrXif2iV"
      },
      "source": [
        "complex_Labels = np.zeros((len(Complex_syl)))\n",
        "freq_Labels = np.ones((len(Frequency_steps_syl)))\n",
        "composite_Labels = 2*np.ones((len(Composite_syl)))\n",
        "Noise_Labels = 3*np.ones((len(Noise_syl)))\n",
        "#Two_Labels = 4*np.ones((len(two_syllable_syl)))\n",
        "\n",
        "Data = np.concatenate((Complex_syl, Frequency_steps_syl, Composite_syl, Noise_syl))\n",
        "Labels = np.concatenate((complex_Labels, freq_Labels, composite_Labels, Noise_Labels))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(Data, Labels, test_size=0.15, random_state=42)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.17, random_state=42)\n",
        "train_images = train_images.astype('float32')\n",
        "train_images = train_images/255\n",
        "val_images = val_images.astype('float32')\n",
        "val_images = val_images/255\n",
        "test_images = test_images.astype('float32')\n",
        "test_images = test_images/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUEpHDf1co2I"
      },
      "source": [
        "print(len(Complex_syl))\n",
        "print(len(Frequency_steps_syl))\n",
        "print(len(Composite_syl))\n",
        "print(len(Noise_syl))\n",
        "#print(len(two_syllable_syl))\n",
        "tot = len(Complex_syl) + len(Frequency_steps_syl) + len(Composite_syl) + len(Noise_syl)\n",
        "Complex_weight = (1/len(Complex_syl))*(tot)/4\n",
        "Frequency_weight = (1/len(Frequency_steps_syl))*(tot)/4\n",
        "Composite_weight = (1/len(Composite_syl))*(tot)/4\n",
        "Noise_weight = (1/len(Noise_syl))*(tot)/4\n",
        "#Two_weight = (1/len(two_syllable_syl))*(tot)/5\n",
        "\n",
        "class_weight = {0:Complex_weight, 1:Frequency_weight, 2:Composite_weight, 3:Noise_weight}\n",
        "print(class_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ore4kDdZp7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af179e0-b9ac-42db-860c-9f148a642103"
      },
      "source": [
        "feature_extractor_model = \"https://tfhub.dev/google/imagenet/mobilenet_v1_100_128/feature_vector/4\"\n",
        "\n",
        "!pip install \"tensorflow>=1.7.0\"\n",
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-hub\n",
        "\n",
        "feature_extractor_layer = hub.KerasLayer(feature_extractor_model, trainable=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=1.7.0 in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.6.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.1.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.39.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.12)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (5.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.7.0) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=1.7.0) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=1.7.0) (1.34.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.7.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.7.0) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.7.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=1.7.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=1.7.0) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=1.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=1.7.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=1.7.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=1.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=1.7.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=1.7.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=1.7.0) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhPCw01wwC_R"
      },
      "source": [
        "#model = tf.keras.Sequential([\n",
        "#                             feature_extractor_layer,\n",
        "#                             tf.keras.layers.Dense(64, activation='relu'), \n",
        "#                             tf.keras.layers.Dense(4)\n",
        "#])\n",
        "#model.build([None, 64, 64, 3])\n",
        "\n",
        "inputs = Input(shape=(64, 64, 3))\n",
        "x = feature_extractor_layer(inputs, training=False)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "#x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "#x = tf.keras.layers.Dropout(0.3)(x)\n",
        "#x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "#x = tf.keras.layers.Dropout(0.3)(x)\n",
        "outputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz02AG32SVMg"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "datagen = ImageDataGenerator(\n",
        "        width_shift_range=0.02,\n",
        "        height_shift_range=0.02,\n",
        "        fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "dZvuZCAPxml5",
        "outputId": "8a987c33-77f9-4391-916d-14d0ee915007"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, shuffle=True, validation_data=(val_images, val_labels), epochs=20, class_weight=class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d6e6dfe968af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWU1OcGTJ8ht",
        "outputId": "c3526a62-e792-463c-acae-502e56385958"
      },
      "source": [
        "model.evaluate(test_images, test_labels, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 - 2s - loss: 0.8335 - accuracy: 0.6881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8335346579551697, 0.6880907416343689]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLIIUSSUmKXI",
        "outputId": "ff8b03de-a368-4005-be02-a874386a7595"
      },
      "source": [
        "feature_extractor_layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, shuffle=True, validation_data=(val_images, val_labels), epochs=10, class_weight=class_weight)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "78/78 [==============================] - 60s 741ms/step - loss: 2.0151 - accuracy: 0.4148 - val_loss: 1.0234 - val_accuracy: 0.6863\n",
            "Epoch 2/10\n",
            "78/78 [==============================] - 57s 732ms/step - loss: 1.0360 - accuracy: 0.6646 - val_loss: 0.8648 - val_accuracy: 0.7196\n",
            "Epoch 3/10\n",
            "78/78 [==============================] - 57s 727ms/step - loss: 0.9044 - accuracy: 0.7344 - val_loss: 0.8144 - val_accuracy: 0.7392\n",
            "Epoch 4/10\n",
            "78/78 [==============================] - 58s 741ms/step - loss: 0.9573 - accuracy: 0.6977 - val_loss: 1.0023 - val_accuracy: 0.6176\n",
            "Epoch 5/10\n",
            "78/78 [==============================] - 58s 737ms/step - loss: 0.9239 - accuracy: 0.7111 - val_loss: 0.8447 - val_accuracy: 0.7294\n",
            "Epoch 6/10\n",
            "78/78 [==============================] - 57s 736ms/step - loss: 0.8637 - accuracy: 0.7401 - val_loss: 0.9221 - val_accuracy: 0.7176\n",
            "Epoch 7/10\n",
            "78/78 [==============================] - 60s 771ms/step - loss: 0.8029 - accuracy: 0.7618 - val_loss: 0.8839 - val_accuracy: 0.7000\n",
            "Epoch 8/10\n",
            "78/78 [==============================] - 57s 733ms/step - loss: 0.8644 - accuracy: 0.7326 - val_loss: 0.7540 - val_accuracy: 0.7804\n",
            "Epoch 9/10\n",
            "78/78 [==============================] - 56s 724ms/step - loss: 0.7602 - accuracy: 0.7840 - val_loss: 0.7492 - val_accuracy: 0.7667\n",
            "Epoch 10/10\n",
            "78/78 [==============================] - 56s 721ms/step - loss: 0.7598 - accuracy: 0.7834 - val_loss: 0.6557 - val_accuracy: 0.8157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcb3eee5518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPKI2-p5v4SW",
        "outputId": "3b1450ec-377c-45e2-9b02-4e0097827d6d"
      },
      "source": [
        "model.evaluate(test_images, test_labels, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17/17 - 2s - loss: 0.6877 - accuracy: 0.8053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6877055168151855, 0.8052930235862732]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}